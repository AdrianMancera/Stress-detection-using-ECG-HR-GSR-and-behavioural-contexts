# -*- coding: utf-8 -*-
"""Major Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nPDPbOtMk37gtKeuPqGqUg5YHkTkmCK5
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.metrics import r2_score

df=pd.read_csv('ECG_GSR_DATASET.csv')

df.head()

df.columns.values.tolist()

# Drop last two columns 
df.drop(df.columns[[-1,-2]],axis=1,inplace=True)

df.head()

df.info()

df.describe()

df.columns.values.tolist()

df.dtypes

df['stress']=pd.to_numeric(df['stress'],errors='coerce')

df.dtypes

df.isnull().sum()

sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='YlGnBu')

df['handGSR']=df['handGSR'].fillna(df['handGSR'].mean())

df=df.dropna()

df.isnull().sum()

sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='YlGnBu')

col=['Lying  down','Sleeping','With friends','Shopping','Running','Excersise','At gym','At party','Bicycling','At main workplace','Watching TV','Surfing the Internet','Computer Work','Phone In Hand','In a meeting','Lab work','In class','Drinking(Alcohol)']
for c in col:
    df[c]=df[c].astype(int)

df.dtypes

df.shape

sns.heatmap(df.corr())

l=['Lying  down','Sleeping','With friends','Shopping','Running','Excersise','At gym','At party','Bicycling','At main workplace','Watching TV','Surfing the Internet','Computer Work','Phone In Hand','In a meeting','Lab work','In class','Drinking(Alcohol)']
for i in l:
    print(df[i].value_counts()) 
    print("\n")

x=df.drop('stress',axis=1)
y=df['stress']

x.head()

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=5)

from sklearn.linear_model import LinearRegression
reg=LinearRegression()
reg.fit(x_train,y_train)

y_pred= reg.predict(x_test)
y_pred=np.array(y_pred)

y_pred

score=reg.score(x_test,y_test)
print(score*100)

y_test=np.array(y_test)

y_pred

r2_score(y_test,y_pred)

predicted = reg.predict([[0.059414634,84.51219512,5.620219512,8.783365854,0,0,0,0,0,1,0,0,0,1,0,0,1,0,1,0,1,0]])
print(predicted)

"""# Neural Network"""

from keras.callbacks import ModelCheckpoint
from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error 
from matplotlib import pyplot as plt
import seaborn as sb
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import warnings 
warnings.filterwarnings('ignore')
warnings.filterwarnings('ignore', category=DeprecationWarning)
from xgboost import XGBRegressor

import tensorflow.compat.v2 as tf
import keras
from keras.models import Sequential 
from keras.models import Sequential
NN_model = Sequential()

# The Input Layer :
NN_model.add(Dense(128, kernel_initializer='normal',input_dim = x.shape[1], activation='relu'))

# The Hidden Layers :
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))

# The Output Layer :
NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))

# Compile the network :
NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])
NN_model.summary()

NN_model.fit(x_train,y_train, epochs=500, batch_size=32, validation_split = 0.2)

y_pred1=NN_model.predict(x_test)

y_pred1

y_pred1=y_pred1.flatten()
y_pred1

r2_score(y_test, y_pred1)

"""# RandomForest Regressor"""

model1= RandomForestRegressor()
model1.fit(x_train,y_train)

y_pred2=model1.predict(x_test)
y_pred2

r2_score(y_test, y_pred2)

"""# XGBoost"""

XGBModel = XGBRegressor()
XGBModel.fit(x_train,y_train , verbose=False)

y_pred3=XGBModel.predict(x_test)
y_pred3

r2_score(y_test, y_pred3)

print(y)

"""## Saving Best Accuracy Scored Model"""

# saving and loading the .h5 model
 
# save model
from keras.models import load_model
NN_model.save('RegressionModel.h5')

print('Model Saved!')
 
# load model

savedModel=load_model('RegressionModel.h5')
savedModel.summary()

y_pred4=savedModel.predict(x_test)

y_pred4=y_pred4.flatten()
y_pred4

r2_score(y_test, y_pred4)

Xnew = np.array([[0.141853659,79.82926829,9.272121951,8.319121951,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]])

ynew = savedModel.predict(Xnew)

print("X=%s\n Predicted Output=%s" % (Xnew[0], ynew[0]))

Xnew = np.array([[0.13395122,80.56097561,11.80765854,9.233170732,1,0,0,0,0,0,0,0,0,1,0,0,1,1,0,1,0,0]])

ynew = savedModel.predict(Xnew)

print("X=%s\n Predicted Output=%s" % (Xnew[0], ynew[0]))

"""# CLASSIFICATION"""

d=df

d.head()

new_stress=pd.cut(d.stress,bins=2,labels=['0','1'])
d.insert(22,'Stress',new_stress)

new_stress

d.head()

d.drop(d.columns[[-1]],axis=1,inplace=True)

d.head()

d.columns

d.isnull().sum()

x1=d.drop('Stress',axis=1)
y1=d['Stress']

x1.shape

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size=0.2, random_state=5)

"""# KNN """

from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
knnclassifier = KNeighborsClassifier(n_neighbors=5)
knnclassifier.fit(x_train,y_train)
y_pred = knnclassifier.predict(x_test)
metrics.accuracy_score(y_test,y_pred)

from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(knnclassifier,x_test,y_test)
plt.title('Confusion Matrix')

from sklearn.model_selection import cross_val_score
knnclassifier = KNeighborsClassifier(n_neighbors=4)
print(cross_val_score(knnclassifier, x_train, y_train, cv=10, scoring ='accuracy').mean())

print(cross_val_score(knnclassifier, x_test, y_test, cv=10, scoring ='accuracy').mean())

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression
classifier=LogisticRegression()
classifier.fit(x_train,y_train)

y_pred = classifier.predict(x_test)
y_pred

# Accuacy
from sklearn.metrics import confusion_matrix, accuracy_score
print(confusion_matrix(y_test,y_pred))
print(accuracy_score(y_test,y_pred))

print("Accuracy :" , classifier.score(x_test,y_test))

classifier.predict(np.array([[-0.0016,97.88888889,3.028511111,20.05544444,0,0,1,0,0,1,0,0,0,0,0,0,1,1,0,1,0,0]]))[0]

from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(classifier,x_test,y_test)
plt.title('Confusion Matrix')

"""# Naive Bayes

"""

from sklearn.naive_bayes import GaussianNB
model=GaussianNB()
model.fit(x_train,y_train)

y_pred=model.predict(x_test)
y_pred

accuracy=accuracy_score(y_test,y_pred)*100
print("Accuracy: ",accuracy)

from sklearn import metrics
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test,y_pred)

plot_confusion_matrix(model,x_test,y_test)
plt.title('Confusion Matrix')

"""# SVM"""

from sklearn.svm import SVC
classifier = SVC(kernel = 'rbf', gamma=0.7)
classifier.fit(x_train, y_train)

y_pred = classifier.predict(x_test)

accuracy=accuracy_score(y_test,y_pred)*100 
print("Accuracy: ",accuracy)

classifier = SVC(kernel = 'linear', random_state=0)
classifier.fit(x_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(x_test)

from sklearn.metrics import accuracy_score
accuracy=accuracy_score(y_test,y_pred)*100
print("Accuracy: ",accuracy)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
cm

plot_confusion_matrix(classifier,x_test,y_test)
plt.title('Confusion Matrix')

"""# DECISION TREE CLASSIFIER"""

from sklearn.tree import DecisionTreeClassifier 

clf = DecisionTreeClassifier()

# Train Decision Tree Classifer
clf = clf.fit(x_train,y_train)

#Predict the response for test dataset
y_pred = clf.predict(x_test)

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

plot_confusion_matrix(clf,x_test,y_test)
plt.title('Confusion Matrix')

"""# ANN MODEL"""

from keras.callbacks import ModelCheckpoint
from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error 
from matplotlib import pyplot as plt
import seaborn as sb
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import warnings 
warnings.filterwarnings('ignore')
warnings.filterwarnings('ignore', category=DeprecationWarning)
from xgboost import XGBRegressor

import tensorflow.compat.v2 as tf
import keras

cl = Sequential()

cl.add(Dense(units=10, input_dim=x1.shape[1], kernel_initializer='uniform', activation='relu'))
 

cl.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))
 

cl.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))
 

cl.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
 

survivalANN_Model=cl.fit(x_train,y_train, batch_size=10 , epochs=10, verbose=1, validation_split=0.2)

# Defining a function for finding best hyperparameters
def FunctionFindBestParams(x_train, y_train):
    
    # Defining the list of hyper parameters to try
    TrialNumber=0
    batch_size_list=[5, 10, 15, 20]
    epoch_list=[5, 10, 50 ,100]
    
    import pandas as pd
    SearchResultsData=pd.DataFrame(columns=['TrialNumber', 'Parameters', 'Accuracy'])
    
    for batch_size_trial in batch_size_list:
        for epochs_trial in epoch_list:
            TrialNumber+=1
            
            
            cl = Sequential()
            cl.add(Dense(units=10, input_dim=x1.shape[1], kernel_initializer='uniform', activation='relu'))
            cl.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))
            cl.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))
            cl.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
 
            
            survivalANN_Model=cl.fit(x_train,y_train, batch_size=batch_size_trial , epochs=epochs_trial, verbose=0)
            
            Accuracy = survivalANN_Model.history['accuracy'][-1]
            
            # printing the results of the current iteration
            print(TrialNumber, 'Parameters:','batch_size:', batch_size_trial,'-', 'epochs:',epochs_trial, 'Accuracy:', Accuracy)
            
            SearchResultsData=SearchResultsData.append(pd.DataFrame(data=[[TrialNumber,
                            'batch_size'+str(batch_size_trial)+'-'+'epoch'+str(epochs_trial), Accuracy]],
                                                                    columns=['TrialNumber', 'Parameters', 'Accuracy'] ))
    return(SearchResultsData)
 
 
# Calling the function
ResultsData=FunctionFindBestParams(x_train, y_train)

# Commented out IPython magic to ensure Python compatibility.

# Printing the best parameter
print(ResultsData.sort_values(by='Accuracy', ascending=False).head(1))
 

# Visualizing the results
# %matplotlib inline
ResultsData.plot(x='Parameters', y='Accuracy', figsize=(15,4), kind='line', rot=20)

history=cl.fit(x_train,y_train, batch_size=5 , epochs=100, verbose=1,validation_split=0.2)

loss_train = history.history['loss']
loss_val = history.history['val_loss']
epochs = range(1,101)
plt.plot(epochs, loss_train, 'r', label='Training loss')
plt.plot(epochs, loss_val, 'b', label='validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

accuracy_train = history.history['accuracy']
accuracy_val = history.history['val_accuracy']
epochs = range(1,101)
plt.plot(epochs, accuracy_train, 'r', label='Training accuracy')
plt.plot(epochs, accuracy_val, 'b', label='validation accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(history.history['accuracy'],'g')
plt.title('Model accuacy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train'],loc='upper left')
plt.show()

plt.plot(history.history['loss'],'y')
plt.title('Model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train'],loc='upper left')
plt.show()

Xnew = np.array([[0.141853659,79.82926829,9.272121951,8.319121951,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]])

ynew = cl.predict_classes(Xnew)

print("X=%s\n Predicted Output=%s" % (Xnew[0], ynew[0]))

Xnew = np.array([[0.13395122,80.56097561,11.80765854,9.233170732,1,0,0,0,0,0,0,0,0,1,0,0,1,1,0,1,0,0]])

ynew = cl.predict_classes(Xnew)

print("X=%s\n Predicted Output=%s" % (Xnew[0], ynew[0]))

# logistic regression for feature importance
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from matplotlib import pyplot
# define dataset
# define the model
model1 = LogisticRegression()
# fit the model
model1.fit(x1, y1)
# get importance
importance = model1.coef_[0]
# summarize feature importance
for i,v in enumerate(importance):
 print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
pyplot.bar([x for x in range(len(importance))], importance)
pyplot.show()

# linear regression feature importance
from sklearn.datasets import make_regression
from sklearn.linear_model import LinearRegression
from matplotlib import pyplot

# define the model
model2 = LinearRegression()
# fit the model
model2.fit(x, y)
# get importance
importance = model2.coef_
# summarize feature importance
for i,v in enumerate(importance):
 print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
pyplot.bar([x for x in range(len(importance))], importance)
pyplot.show()

from sklearn.datasets import make_regression
from sklearn.ensemble import RandomForestRegressor
from matplotlib import pyplot
# define the model
model3 = RandomForestRegressor()
# fit the model
model3.fit(x, y)
# get importance
importance = model3.feature_importances_
# summarize feature importance
for i,v in enumerate(importance):
 print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
pyplot.bar([x for x in range(len(importance))], importance)
pyplot.show()

# saving and loading the .h5 model
 
# save model
from keras.models import load_model
cl.save('ClassificationModel.h5')

print('Model Saved!')
 
# load model

saveModel=load_model('ClassificationModel.h5')
saveModel.summary()

Xnew = np.array([[0.141853659,79.82926829,9.272121951,8.319121951,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]])

ynew = saveModel.predict_classes(Xnew)

print("X=%s\n Predicted Output=%s" % (Xnew[0], ynew[0]))

Xnew = np.array([[0.13395122,80.56097561,11.80765854,9.233170732,1,0,0,0,0,0,0,0,0,1,0,0,1,1,0,1,0,0]])

ynew = saveModel.predict_classes(Xnew)

print("X=%s\n Predicted Output=%s" % (Xnew[0], ynew[0]))